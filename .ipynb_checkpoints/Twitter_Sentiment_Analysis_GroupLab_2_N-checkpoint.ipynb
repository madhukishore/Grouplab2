{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Artificial Intelligence and Machine Learning\n",
    "## A Program by IIIT-H and TalentSprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To be done in the Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this experiment is to perform sentimental analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we will be using twitter dataset as training data and crawled realtime tweets for testing. \n",
    "\n",
    "The Ground truth is 1 for positive tweet and 0 for negative tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few examples of positive and negative tweets are:\n",
    "\n",
    "**Few Positive Tweets: **\n",
    "1.  @Msdebramaye I heard about that contest! Congrats girl!!\n",
    "2. UNC!!! NCAA Champs!! Franklin St.: I WAS THERE!! WILD AND CRAZY!!!!!! Nothing like it...EVER http://tinyurl.com/49955t3\n",
    "\n",
    "**Few Negative Tweets:**\n",
    "1. no more taking Irish car bombs with strange Australian women who can drink like rockstars...my head hurts.\n",
    "2. Just had some bloodwork done. My arm hurts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "\n",
    "https://www.kaggle.com/c/twitter-sentiment-analysis2/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: (2 marks)\n",
    "\n",
    "The first exercise is cleaning the tweets.\n",
    "Perform preprocessing as required.\n",
    "\n",
    "Complete the functon : preprocess_tweets \n",
    "\n",
    "Input or Arguement to the function : tweet as a string \n",
    "\n",
    "Return value: processed tweet as string \n",
    "\n",
    "Hint: Use regular expressions\n",
    "* convert the all the cases into lower case\n",
    "  + look at lower()\n",
    "* Replace any urls with the word \"URL\"\n",
    "  + Hint : \n",
    "      - re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',\"Tweet\") (re is python regular expression package)\n",
    "* convert the username to \"AT_USER\", consider any word that starts with @ as user name\n",
    "  + Hint : \n",
    "      - re.sub('@[^\\s]+','AT_USER',\"Tweet\")\n",
    "* Remove multiple whitespaces with a single white space\n",
    "  + Hint :\n",
    "      - re.sub('[\\s]+', ' ', tweet)\n",
    "* Replace hashtag words (#word) with just the words (word)\n",
    "  + Hint : \n",
    "      - re.sub(r'#([^\\s]+)', r'\\1', \"tweet\")\n",
    "      \n",
    "* TEST CASE :\n",
    "    + given the tweet \"@V_DEL_ROSSI: Me         #dragging myself to the gym https://t.co/cOjM0mBVeY\"\n",
    "    + output should be \"AT_USER me dragging myself to the gym URL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T14:16:37.343616Z",
     "start_time": "2018-06-30T14:16:37.336703Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "# Operating System\n",
    "import os\n",
    "# Regular Expression\n",
    "import re\n",
    "# nltk packages\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# Basic Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# PCA Package\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(tweet):\n",
    "    tweet=tweet.lower()\n",
    "    tweet = re.sub('((www.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    tweet=re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    tweet=re.sub('[\\s]+', ' ', tweet)\n",
    "    tweet=re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    tweet=re.sub('&lt;','',tweet)#xml tag\n",
    "    tweet=re.sub('&amp;','',tweet)\n",
    "    tweet=re.sub(':o','smile',tweet) #replace smileys with good or bad emotion\n",
    "    tweet=re.sub(':-\\|','speechless',tweet)\n",
    "    tweet=re.sub('[.!,-]+', '', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('twitter_train.csv',encoding='latin-1',converters={'SentimentText':preprocess_tweets})\n",
    "pw=pd.read_csv('positive-words.txt').values\n",
    "nw=pd.read_csv('negative-words.txt').values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abundant'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pw[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 ' is so sad for my apl friend']\n",
      "[2 0 ' i missed the new moon trailer']\n",
      "[3 1 ' omg its already 7:30 smile']\n",
      "[4 0\n",
      " \"  omgaga im sooo im gunna cry i've been at this dentist since 11 i was suposed 2 just get a crown put on (30mins)\"]\n",
      "[5 0 ' i think mi bf is cheating on me t_t']\n",
      "[6 0 ' or i just worry too much? ']\n",
      "[7 1 ' juuuuuuuuuuuuuuuuussssst chillin']\n",
      "[8 0 ' sunny again work tomorrow speechless tv tonight']\n",
      "[9 1 ' handed in my uniform today  i miss you already']\n",
      "[10 1 ' hmmmm i wonder how she my number AT_USER']\n",
      "[11 0 ' i must think about positive']\n",
      "[12 1 ' thanks to all the haters up in my face all day 112102']\n",
      "[13 0 ' this weekend has sucked so far']\n",
      "[14 0 ' jb isnt showing in australia any more']\n",
      "[15 0 ' ok thats it you win']\n",
      "[16 0 '  this is the way i feel right now']\n",
      "[17 0\n",
      " \" awhhe man i'm completely useless rt now funny all i can do is twitter URL\"]\n",
      "[18 1\n",
      " \" feeling strangely fine now i'm gonna go listen to some semisonic to celebrate\"]\n",
      "[19 0 ' huge roll of thunder just nowso scary']\n",
      "[20 0\n",
      " \" i just cut my beard off it's only been growing for well over a year i'm gonna start it over AT_USER is happy in the meantime\"]\n",
      "[21 0 ' very sad about iran']\n",
      "[22 0 ' wompppp wompp']\n",
      "[23 1\n",
      " \" you're the only one who can see this cause no one else is following me this is for you because you're pretty awesome\"]\n",
      "[24 0\n",
      " \" sad level is 3 i was writing a massive blog tweet on myspace and my comp shut down now it's all lost *lays in fetal position*\"]\n",
      "[25 0\n",
      " '  headed to hospitol : had to pull out of the golf tourny in 3rd place i think i reripped something  yeah that ']\n",
      "[26 0 ' boring ): whats wrong with him?? please tell me :/']\n",
      "[27 0\n",
      " \" can't be bothered i wish i could spend the rest of my life just sat here and going to gigs seriously\"]\n",
      "[28 0\n",
      " ' feeeling like shit right now i really want to sleep but nooo i have 3 hours of dancing and an art assignment to finish ']\n",
      "[29 1 ' goodbye exams hello alcohol tonight ']\n",
      "[30 0\n",
      " \" i didn't realize it was that deep geez give a girl a warning atleast\"]\n",
      "[31 0\n",
      " ' i hate it when any athlete appears to tear an acl on live television']\n",
      "[32 0\n",
      " \" i miss you guys too i think i'm wearing skinny jeans a cute sweater and heels not really sure what are you doing today\"]\n",
      "[33 0 '  meet your meat URL']\n",
      "[34 0 ' my horsie is moving on saturday morning']\n",
      "[35 0 ' no sat offneed to work 6 days a week ']\n",
      "[36 0\n",
      " ' really dont like doing my room its so boring sick of doing my wardrobe out cant waiit till i have my walk in one yay']\n",
      "[37 0 ' sox floyd was great but relievers need a scolding']\n",
      "[38 0 ' times by like a million']\n",
      "[39 1 ' uploading pictures on friendster ']\n",
      "[40 0\n",
      " \" what type of a spaz downloads a virus? my brother that's who :\\\\ msn is now fucked forever :'(\"]\n",
      "[41 0 ' fightiin wiit the babes']\n",
      "[42 1\n",
      " ' (:   so i wrote something last week and i got a call from someone in the new york office URL']\n",
      "[43 0 ' *enough said*']\n",
      "[44 1\n",
      " '  do i need to even say it? do i? well here i go anyways: chris cornell in chicago  tonight ']\n",
      "[45 1 '  health class (what a joke)']\n",
      "[46 1 ' AT_USER 3 go to the show tonight']\n",
      "[47 0\n",
      " ' AT_USER AT_USER it really makes me sad when i look at muslims reality now']\n",
      "[48 0 '  all time low shall be my motivation for the rest of the week']\n",
      "[49 0\n",
      " ' and the entertainment is over someone complained properly AT_USER experimental you say? he should experiment with a melody']\n",
      "[50 0 \" another year of lakers  that's neither magic nor fun \"]\n",
      "[51 0 ' baddest day eveer ']\n",
      "[52 1 ' bathroom is clean now on to more enjoyable tasks']\n",
      "[53 1 ' boom boom pow']\n",
      "[54 0 \" but i'm proud\"]\n",
      "[55 0 ' congrats to helio though']\n",
      "[56 0\n",
      " ' david must be hospitalized for five days end of july (palatine tonsils) i will probably never see katie in concert ']\n",
      "[57 0 \" friends are leaving me 'cause of this stupid love URL\"]\n",
      "[58 1 ' go give ur mom a hug right now URL']\n",
      "[59 1 ' going to see harry sunday happiness ']\n",
      "[60 0 ' hand quilting it is then']\n",
      "[61 0 ' hate u  leysh t9ar5  =((((((( ']\n",
      "[62 1 '= i always get what i want']\n",
      "[63 1 ' i bend backwards ']\n",
      "[64 1\n",
      " \" i get off work sooooon i miss cody booo haven't seen him in foreverr\"]\n",
      "[65 1\n",
      " \" i hate allergies should i get my hair cut tomorrow? i'm taking a public poll\"]\n",
      "[66 0 '  i love you guys so much that it hurts URL']\n",
      "[67 0 ' i miss earl']\n",
      "[68 0 ' i miss new jersey']\n",
      "[69 0\n",
      " \" i missed the first hour of sytycd last night and i can't find it online\"]\n",
      "[70 0 ' i need a u2 fix now']\n",
      "[71 0 \" i never thought i'd become second choice\"]\n",
      "[72 0 ' i think i may be too friendlylol o well']\n",
      "[73 0 ' i think manuel (my basil plant) only has days to live ']\n",
      "[74 0 ' i wanna be at home @ churchi wonder wht they are doing?']\n",
      "[75 0 ' i wanna make my own pizza']\n",
      "[76 0\n",
      " ' i want a 120gb harddrive or a 37 inch tv or a new guitar anyonefeeling generous? =p x']\n",
      "[77 0 ' i want a hug']\n",
      "[78 0 ' i want miley to tour australia']\n",
      "[79 0\n",
      " ' i wanted to sleep in this morning but a mean kid through a popsicle stick at me head i wish i could fly away like those squirrels']\n",
      "[80 0 ' i was too slow to get $1 up tix']\n",
      "[81 0\n",
      " ' i will send sunshine to northern ireland are you going swimming today AT_USER']\n",
      "[82 0\n",
      " \" i wish i could go to t4 on the beach :'( would be great to see AT_USER  AT_USER \"]\n",
      "[83 0\n",
      " ' i would be so much happier if the walls of my bedroom were painted white']\n",
      "[84 0\n",
      " ' idk wat 2 do who can i trust me im sorry 4 all da pain i have caused nebody ima take dis time out 2 straighten myself out i luv yall']\n",
      "[85 0\n",
      " \" i'm finding the intercept slopeand banging my head against the wallmath brain heads come save me\"]\n",
      "[86 1 \" i'm really going to bed now\"]\n",
      "[87 0 \" im sick 'cough cough'\"]\n",
      "[88 0 ' in cab headed to the airport going home christy&gt;']\n",
      "[89 0\n",
      " \" in case i feel emo in camp (feeling a wee bit of it alr)am bringing in the human rights watch world report 2009hope it'll work\"]\n",
      "[90 1 ' jin has a twitter']\n",
      "[91 0 ' jonas day is almost over ']\n",
      "[92 0\n",
      " \" jus got hom fr tda funeral i'm so sad i cried so much times much love grandpa3 i never got to say my last &quot;goodbye&quot; to him\"]\n",
      "[93 1\n",
      " ' just gonna smilecuz it is what it isand im not sure what more they could want']\n",
      "[94 1\n",
      " \" just got home and i got to see my friend zahra whom i haven't seen since we graduated that makes me so happy\"]\n",
      "[95 0 '  longest night ever ugh URL']\n",
      "[96 0 \" mi momacita won't let me go to my bf's bball game grrr\"]\n",
      "[97 0\n",
      " ' mom says i have to get a new phone immediatelyoff to tmobile she paying']\n",
      "[98 0 ' my new car was stolenby my mother who wanted to go pose at church']\n",
      "[99 0 ' no hang out with the girls 2day 2moro hope so']\n",
      "[100 0\n",
      " ' no movie times for sunday rats will have to plan tomorrow i guess this means i have to work on those two presentations i am doing']\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(data.values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: (3 marks)\n",
    "\n",
    "Tokenize the processed tweets to make a tweet into a list of words and make sure that no punctuations are returned. so that it can be used in the next steps to represent the tweet as a feature vector. Remove the Stops words, if necessary\n",
    "\n",
    "Complete the functon : word_tokenizer \n",
    "\n",
    "Input or Arguement to the function : processed tweet\n",
    "\n",
    "Return value: list of words without any punctuations\n",
    "\n",
    "TEST CASE :\n",
    "\n",
    "Given an input :\n",
    "    \"Neither Man, nor machine can replace its creator. really?.\"\n",
    "    \n",
    "Result : \n",
    "    ['neither', 'man', 'nor', 'machine', 'replace', 'creator', 'hahaha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T15:00:25.764720Z",
     "start_time": "2018-06-29T15:00:25.752064Z"
    }
   },
   "outputs": [],
   "source": [
    "stopWords = pd.read_csv('stopwords.txt').values\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "vocabulary = set([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T15:10:31.107306Z",
     "start_time": "2018-06-29T15:10:31.097419Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_tokenizer(tweet):\n",
    "    words = re.findall(r'(\\b[A-Za-z][a-z]{2,15}\\b)', tweet)\n",
    "    words = [ stemmer.stem(word.lower()) for word in words if not word.lower() in stopWords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99989"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.values.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(data.values.shape[0]):\n",
    "    words= word_tokenizer(data.values[i][2])\n",
    "    for word in words:\n",
    "          vocabulary.add(word)\n",
    "vocabularyList=list(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add postive words count, negative wourd count, neutral, and tweet (label)\n",
    "voclength=len(vocabulary)\n",
    "fvlength=voclength+4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: (5 marks)\n",
    "\n",
    "Using the list of words from the above the step, \n",
    "* represent the tweet as a feature vector using bag of words/Word2vec\n",
    "\n",
    "Hint : counts of postive/negative/neutral words as three features can also be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T15:15:36.672042Z",
     "start_time": "2018-06-29T15:15:36.666906Z"
    }
   },
   "outputs": [],
   "source": [
    "def getfeaturevector(tokenized_tweet,sentiment):\n",
    "    feature_vector=np.zeros(fvlength)\n",
    "    for word in tokenized_tweet: \n",
    "        if vocabularyList.index(word) !=-1:\n",
    "            feature_vector[vocabularyList.index(word)]+=1\n",
    "        flag=0\n",
    "        for i in range(len(pw)):\n",
    "            if word == pw[i][0]:\n",
    "                feature_vector[voclength]+=1\n",
    "                flag=1\n",
    "        for i in range(len(nw)):\n",
    "            if word == nw[i][0]:\n",
    "                feature_vector[voclength+1]+=1\n",
    "                flag=-1\n",
    "        if flag==0:\n",
    "            feature_vector[voclength+2]+=1\n",
    "        feature_vector[voclength+3]=sentiment\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fv = getfeaturevector(word_tokenizer(data.values[20][2]))\n",
    "#fv[2133],fv[908],fv[len(vocabulary)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(data.values):\n",
    "#    fv = getfeaturevector(word_tokenizer(data.values[20][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.values[20][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldata=[]\n",
    "for i in range(data.values.shape[0]):\n",
    "    finaldata.append(getfeaturevector(word_tokenizer(data.values[i][2]),data.values[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: (Marks : 5 ) \n",
    "\n",
    "\n",
    "Load the given training data and use the above functions you created to process, to tokenise and to get feature vector.\n",
    "\n",
    "Considering the feature vector as input to the classifier, Train a classifier to classify the sentiment of the tweet correctly.\n",
    "\n",
    "Divide the training data into two sets, to validate your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print\n",
    "# Your Code Hereimport random\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "def randomize():\n",
    "    TRAIN_TEST_RATIO = 0.8\n",
    "    data = finaldata\n",
    "    #print(data)\n",
    "    for one in data:\n",
    "        #print(one)\n",
    "        if random.random() < TRAIN_TEST_RATIO:\n",
    "            train.append(one)\n",
    "        else:\n",
    "            test.append(one)\n",
    "    print (test[:10])\n",
    "    \n",
    "randomize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import collections\n",
    "def dist(a, b):\n",
    "    sqSum = 0\n",
    "    for i in np.arange( len(a)):\n",
    "        sqSum += (a[i] - b[i]) ** 2\n",
    "    return math.sqrt(sqSum)\n",
    "# ------------------------------------------------ #\n",
    "# We are assuming that the label is the last field #\n",
    "# If not, munge the data to make it so!            #\n",
    "# ------------------------------------------------ #\n",
    "def kNN(k, train, given):\n",
    "    distances = []\n",
    "    for t in train:\n",
    "        distance=(dist(t[:-1], given), str(int(t[-1])))\n",
    "        #print(distance)\n",
    "        distances.append(distance)\n",
    "    distances.sort()\n",
    "    return distances[:k]\n",
    "\n",
    "def kNN_classify(k, train, given):\n",
    "    tally = collections.Counter()\n",
    "    #print(tally)\n",
    "    for nn in kNN(k, train, given):\n",
    "        tally.update(nn[-1])\n",
    "    return tally.most_common(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_accuracy(Kvalue):\n",
    "    num = 0\n",
    "    print(\"calculating accuracy\")\n",
    "    for t in test:\n",
    "        prediction = kNN_classify(Kvalue, train, t)[0]\n",
    "        #print(\"prediction, label is\", prediction, t[134])\n",
    "        if (int(prediction) == int(t[index])):\n",
    "            num += 1\n",
    "        #print(kNN_classify(5, train, t)[0], t[-1])\n",
    "    #print(num/len(test))\n",
    "    return (num/len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_accuracy(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: (Marks : 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter crawling using tweepy\n",
    "\n",
    "Use tweepy to get the tweets on real time, which is used as test data for the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter account\n",
    "\n",
    "Create a twitter account if you don't have one by going to the link given below:\n",
    "\n",
    "https://twitter.com/i/flow/signup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweepy: tweepy is the python client for the official Twitter API.\n",
    "Install it using following pip command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets need to be gathered so as to perform Sentiment analysis on those tweets. They can be fetched from Twitter using the Twitter API. \n",
    "\n",
    "In order to fetch tweets through Twitter API, one needs to register an App through their twitter account. Follow these steps for the same:\n",
    "<ul>\n",
    "<li>Open the link given below to create a App through the twitter account.\n",
    "    https://apps.twitter.com\n",
    "<li>click the button: ‘Create New App’\n",
    "<li>Fill the application details. You can leave the callback url field empty.\n",
    "<li>Once the app is created, you will be redirected to the app page.\n",
    "<li>Open the ‘Keys and Access Tokens’ tab.\n",
    "<li>Copy ‘Consumer Key’, ‘Consumer Secret’, ‘Access token’ and ‘Access Token Secret’.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T14:11:04.892431Z",
     "start_time": "2018-06-29T14:11:04.881562Z"
    }
   },
   "outputs": [],
   "source": [
    "#Replace with your ‘Consumer Key’, ‘Consumer Secret’, ‘Access token’ and ‘Access Token Secret’ below. \n",
    "\n",
    "consumer_key = \"sjqCktpagvR4NjTsUR3DcA7pp\"\n",
    "consumer_secret = \"dqxiWPhqt7n7uR0tGqA18bPCRsXxtEPVNg3HmOtfmzjjjMEXwa\"\n",
    "access_token =  \"968060979681681408-kTK6GHKeBRr3oyEalq3NwQKtC9Vn4fS\"\n",
    "access_token_secret = \"pICNG5EIzlBwf0vmSLCitRmtloKr1J1Hmd6w8JixWDmKU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to authenticate your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T14:11:07.156485Z",
     "start_time": "2018-06-29T14:11:06.815300Z"
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweepy Cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code gives the search results from twitter for the search string passed to the keyword arguement \"q\" in the tweepy.Cursor. The number passed to the items method of tweepy.Cursor indicates that it gives 100 such tweets, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T14:18:19.093637Z",
     "start_time": "2018-06-29T14:18:07.691456Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tweepy.Cursor(api.search, q='bankofamerica', lang = 'en', full_text=True).items(100):\n",
    "    print(i._json['text'])\n",
    "    #print(myStreamListener.processTweet(i._json['text']), end='\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also apply the preprocessing steps and obtain the feature vectors for the crawled twitter data.\n",
    "Classify the crawled tweets by passing its feature vector to the trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
